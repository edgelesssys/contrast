"use strict";(globalThis.webpackChunkcontrast_docs=globalThis.webpackChunkcontrast_docs||[]).push([[30154],{28453(e,n,s){s.d(n,{R:()=>l,x:()=>a});var t=s(96540);const r={},i=t.createContext(r);function l(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(i.Provider,{value:n},e.children)}},95921(e,n,s){s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"howto/cluster-setup/bare-metal","title":"Prepare a bare-metal instance","description":"Prerequisites","source":"@site/docs/howto/cluster-setup/bare-metal.md","sourceDirName":"howto/cluster-setup","slug":"/howto/cluster-setup/bare-metal","permalink":"/contrast/pr-preview/pr-2092/next/howto/cluster-setup/bare-metal","draft":false,"unlisted":false,"editUrl":"https://github.com/edgelesssys/contrast/edit/main/docs/docs/howto/cluster-setup/bare-metal.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"Workload deployment","permalink":"/contrast/pr-preview/pr-2092/next/getting-started/deployment"},"next":{"title":"Install CLI","permalink":"/contrast/pr-preview/pr-2092/next/howto/install-cli"}}');var r=s(74848),i=s(28453);const l={},a="Prepare a bare-metal instance",o={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware and firmware setup",id:"hardware-and-firmware-setup",level:2},{value:"Kernel setup",id:"kernel-setup",level:2},{value:"Kubernetes cluster setup",id:"kubernetes-cluster-setup",level:2},{value:"K3s",id:"k3s",level:3},{value:"Preparing a cluster for GPU usage",id:"preparing-a-cluster-for-gpu-usage",level:2},{value:"Supported GPU hardware",id:"supported-gpu-hardware",level:3},{value:"Setup",id:"setup",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components},{Details:s,TabItem:t,Tabs:l}=n;return s||u("Details",!0),t||u("TabItem",!0),l||u("Tabs",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"prepare-a-bare-metal-instance",children:"Prepare a bare-metal instance"})}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(l,{queryString:"vendor",children:[(0,r.jsx)(t,{value:"amd",label:"AMD SEV-SNP",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A supported CPU:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AMD Epyc 7003 series (Milan)"}),"\n",(0,r.jsx)(n.li,{children:"AMD Epyc 9004 series (Genoa)"}),"\n"]}),"\n"]}),"\n"]})}),(0,r.jsx)(t,{value:"intel",label:"Intel TDX",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A supported CPU:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"5th Gen Intel Xeon Scalable Processor"}),"\n",(0,r.jsx)(n.li,{children:"Intel Xeon 6 Processors"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Platform must fulfill the ",(0,r.jsx)(n.a,{href:"https://cc-enabling.trustedservices.intel.com/intel-tdx-enabling-guide/03/hardware_selection/#dimm-ie-main-memory-requirements",children:"DIMM requirements"}),"."]}),"\n"]})})]}),"\n",(0,r.jsx)(n.h2,{id:"hardware-and-firmware-setup",children:"Hardware and firmware setup"}),"\n",(0,r.jsxs)(l,{queryString:"vendor",children:[(0,r.jsxs)(t,{value:"amd",label:"AMD SEV-SNP",children:[(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Update your BIOS to a version that supports AMD SEV-SNP. Updating to the latest available version is recommended as newer versions will likely contain security patches for AMD SEV-SNP."}),"\n",(0,r.jsx)(n.li,{children:"Enter BIOS setup to enable SMEE, IOMMU, RMP coverage, and SEV-SNP. Set the SEV-ES ASID Space Limit to a non-zero number (higher is better)."}),"\n",(0,r.jsxs)(n.li,{children:["Download the latest firmware version for your processor from ",(0,r.jsx)(n.a,{href:"https://www.amd.com/de/developer/sev.html",children:"AMD"}),", unpack it, and place it in ",(0,r.jsx)(n.code,{children:"/lib/firmware/amd"}),"."]}),"\n"]}),(0,r.jsxs)(n.p,{children:["Consult AMD's ",(0,r.jsx)(n.a,{href:"https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58207-using-sev-with-amd-epyc-processors.pdf",children:"Using SEV with AMD EPYC Processors user guide"})," for more information."]})]}),(0,r.jsxs)(t,{value:"intel",label:"Intel TDX",children:[(0,r.jsxs)(n.p,{children:["Follow Canonical's instructions in ",(0,r.jsx)(n.a,{href:"https://github.com/canonical/tdx?tab=readme-ov-file#43-enable-intel-tdx-in-the-hosts-bios",children:"4.3 Enable Intel TDX in the Host's BIOS"})," and ",(0,r.jsx)(n.a,{href:"https://github.com/canonical/tdx?tab=readme-ov-file#92-setup-intel-sgx-data-center-attestation-primitives-intel-sgx-dcap-on-the-host-os",children:"9.2 Setup Intel\xae SGX Data Center Attestation Primitives (Intel\xae SGX DCAP) on the Host OS"})," except for 9.2.1."]}),(0,r.jsx)(n.p,{children:"Instead of step 9.2.1, run the following:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"sudo add-apt-repository ppa:kobuk-team/tdx-attestation-release\nsudo sed -i 's/questing/plucky/g' /etc/apt/sources.list.d/kobuk-team-ubuntu-tdx-attestation-release-*.sources\napt update\napt install sgx-dcap-pccs tdx-qgs libsgx-dcap-default-qpl sgx-ra-service sgx-pck-id-retrieval-tool\n"})}),(0,r.jsx)(n.p,{children:"You can ignore the other sections of the document."}),(0,r.jsxs)(n.p,{children:["Follow Intel's guide to ",(0,r.jsx)(n.a,{href:"https://cc-enabling.trustedservices.intel.com/intel-tdx-enabling-guide/04/hardware_setup/#update-intel-tdx-module-via-binary-deployment",children:"Update Intel TDX Module via Binary Deployment"}),".\nIntel recommends to install the latest TDX module version available."]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"kernel-setup",children:"Kernel setup"}),"\n",(0,r.jsxs)(l,{queryString:"vendor",children:[(0,r.jsx)(t,{value:"amd",label:"AMD SEV-SNP",children:(0,r.jsx)(n.p,{children:"Install Linux kernel 6.11 or greater."})}),(0,r.jsx)(t,{value:"intel",label:"Intel TDX",children:(0,r.jsxs)(n.p,{children:["Install Ubuntu 25.10 and leave the kernel at the default version. Other distributions with a 6.16+ kernel might work as well, but we currently only provide support for Ubuntu 25.10.\nAdd the ",(0,r.jsx)(n.code,{children:"nohibernate"})," and ",(0,r.jsx)(n.code,{children:"kvm_intel.tdx=1"})," kernel command line parameters, for example by updating ",(0,r.jsx)(n.code,{children:"GRUB_CMDLINE_LINUX"})," in ",(0,r.jsx)(n.code,{children:"/etc/default/grub"}),"."]})})]}),"\n",(0,r.jsxs)(n.p,{children:["Containerd uses a significant amount of ",(0,r.jsx)(n.code,{children:"inotify"})," instances, so we recommend to allow at least 8192.\nIf necessary, the default can be increased by creating a config override file (for example in ",(0,r.jsx)(n.code,{children:"/etc/sysctl.d/98-containerd.conf"}),") with the following content:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ini",children:"fs.inotify.max_user_instances = 8192\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Apply this change by running ",(0,r.jsx)(n.code,{children:"systemctl restart systemd-sysctl"})," and verify it using ",(0,r.jsx)(n.code,{children:"sysctl fs.inotify.max_user_instances"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"kubernetes-cluster-setup",children:"Kubernetes cluster setup"}),"\n",(0,r.jsx)(n.p,{children:"Contrast can be deployed with different Kubernetes distributions."}),"\n",(0,r.jsx)(n.h3,{id:"k3s",children:"K3s"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://k3s.io/",children:"K3s"})," is a lightweight Kubernetes distribution that's easy to set up."]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Follow the ",(0,r.jsx)(n.a,{href:"https://docs.k3s.io/",children:"K3s setup instructions"})," to create a cluster.\nContrast is currently tested with K3s version ",(0,r.jsx)(n.code,{children:"v1.34.1+k3s1"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Install a block storage provider such as ",(0,r.jsx)(n.a,{href:"https://longhorn.io/docs/1.9.1/deploy/install/install-with-helm/",children:"Longhorn"})," and mark it as the default storage class."]}),"\n",(0,r.jsxs)(n.li,{children:["Ensure that a load balancer controller is installed.\nFor development and testing purposes, the built-in ",(0,r.jsx)(n.a,{href:"https://docs.k3s.io/networking/networking-services#service-load-balancer",children:"ServiceLB"})," should suffice."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Then, install the ConfigMap to configure the Contrast node-installer for use with K3s:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"kubectl apply -f https://github.com/edgelesssys/contrast/releases/latest/download/node-installer-target-config-k3s.yml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["If you need to pull large images, configure K3s to use a longer ",(0,r.jsx)(n.code,{children:"runtime-request-timeout"})," duration than the ",(0,r.jsx)(n.a,{href:"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/",children:"default value of 2 minutes"})," used by the kubelet,\nfor example by setting"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubelet-arg:\n  - "runtime-request-timeout=5m"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["in ",(0,r.jsx)(n.code,{children:"/etc/rancher/k3s/config.yaml"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"preparing-a-cluster-for-gpu-usage",children:"Preparing a cluster for GPU usage"}),"\n",(0,r.jsx)(n.h3,{id:"supported-gpu-hardware",children:"Supported GPU hardware"}),"\n",(0,r.jsx)(n.p,{children:"Contrast can only be used with the following Confidential Computing enabled GPUs:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX B200, 8-GPU, SXM6 180GB HBM3e, AC"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX B200-850, 8-GPU, SXM6 180GB HBM3e, AC"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H100 4-GPU 64GB HBM2e (Partner Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H100 4-GPU 80GB HBM3 (Partner Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H100 4-GPU 94GB HBM2e (Partner Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H100 8-GPU 80GB (Air Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H100 8-GPU 96GB (Air Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H20 141GB HBM3e 8-GPU (Air Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H200 8-GPU 141GB (Air Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H20A HBM3 96GB 8-GPU (Air Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H800 8-GPU 80GB (Air Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA HGX H800 8-GPU 80GB (Partner Cooled)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA H100 NVL"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA H100 PCIe"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA H200 NVL"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA H800 NVL"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA H800 PCIe"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA RTX PRO 6000 Blackwell Server Edition"}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["Currently, only the ",(0,r.jsx)(n.code,{children:"NVIDIA H100 PCIe"})," and ",(0,r.jsx)(n.code,{children:"NVIDIA HGX B200"})," models are covered by tests. Other GPUs aren't guaranteed to work."]})}),"\n",(0,r.jsx)(n.p,{children:"To check what GPUs are available on your system, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"lspci -nnk | grep '3D controller' -A3\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell-session",children:"41:00.0 3D controller [0302]: NVIDIA Corporation GH100 [H100 PCIe] [10de:2331] (rev a1)\n   Subsystem: NVIDIA Corporation GH100 [H100 PCIe] [10de:1626]\n   Kernel driver in use: vfio-pci\n   Kernel modules: nvidiafb, nouveau\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Further information is provided in ",(0,r.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/secure-ai-compatibility-matrix/",children:"NVIDIA's Secure AI Compatibility Matrix"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,r.jsxs)(n.p,{children:["To enable GPU usage on a Contrast cluster, some conditions need to be fulfilled for ",(0,r.jsx)(n.em,{children:"each cluster node"})," that should host GPU workloads:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"You must activate the IOMMU. You can check by running:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"ls /sys/kernel/iommu_groups\n"})}),"\n",(0,r.jsxs)(n.p,{children:["If the output contains the group indices (",(0,r.jsx)(n.code,{children:"0"}),", ",(0,r.jsx)(n.code,{children:"1"}),", ...), the IOMMU is supported on the host.\nOtherwise, add ",(0,r.jsx)(n.code,{children:"intel_iommu=on"})," to the kernel command line."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Additionally, the host kernel needs to have the following kernel configuration options enabled:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"CONFIG_VFIO"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"CONFIG_VFIO_IOMMU_TYPE1"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"CONFIG_VFIO_MDEV"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"CONFIG_VFIO_MDEV_DEVICE"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"CONFIG_VFIO_PCI"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://releases.ubuntu.com/questing/",children:"Ubuntu Server 25.10"}),", for example, fulfills these requirements out-of-the-box, and no further changes are necessary here."]}),"\n",(0,r.jsxs)(n.p,{children:["If the per-node requirements are fulfilled, deploy the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest",children:"NVIDIA GPU Operator"})," to the cluster. It provisions pod-VMs with GPUs via VFIO."]}),"\n",(0,r.jsx)(n.p,{children:"For a GPU-enabled Contrast cluster, you can then deploy the operator with the following commands:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"# Add the NVIDIA Helm repository\nhelm repo add nvidia https://helm.ngc.nvidia.com/nvidia && helm repo update\n\n# Install the GPU Operator\nhelm install --wait --generate-name \\\n  -n gpu-operator --create-namespace \\\n  nvidia/gpu-operator \\\n  --version=v25.10.1 \\\n  --set sandboxWorkloads.enabled=true \\\n  --set sandboxWorkloads.defaultWorkload=vm-passthrough \\\n  --set kataManager.enabled=true \\\n  --set kataManager.config.runtimeClasses=null \\\n  --set kataManager.repository=nvcr.io/nvidia/cloud-native \\\n  --set kataManager.image=k8s-kata-manager \\\n  --set kataManager.version=v0.2.4 \\\n  --set ccManager.enabled=true \\\n  --set ccManager.defaultMode=on \\\n  --set ccManager.repository=nvcr.io/nvidia/cloud-native \\\n  --set ccManager.image=k8s-cc-manager \\\n  --set ccManager.version=v0.2.0 \\\n  --set sandboxDevicePlugin.repository=ghcr.io/nvidia \\\n  --set sandboxDevicePlugin.image=nvidia-sandbox-device-plugin \\\n  --set sandboxDevicePlugin.version=8e76fe81 \\\n  --set 'sandboxDevicePlugin.env[0].name=P_GPU_ALIAS' \\\n  --set 'sandboxDevicePlugin.env[0].value=pgpu' \\\n  --set nfd.enabled=true \\\n  --set nfd.nodefeaturerules=true\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Refer to the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html",children:"official installation instructions"})," for details and further options."]}),"\n",(0,r.jsxs)(n.p,{children:["To enable support for Blackwell GPUs, some additional steps are necessary. For Hopper GPUs (for example the ",(0,r.jsx)(n.code,{children:"NVIDIA H100 PCIe"}),"), these steps can be skipped."]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"Enabling Support for Blackwell GPUs"}),(0,r.jsx)(n.p,{children:"First, gather the Blackwell device ID:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"lspci -nnk | grep '3D controller' -A3\n"})}),(0,r.jsx)(n.p,{children:"Which yields output like this:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell-session",children:"0000:18:00.0 3D controller [0302]: NVIDIA Corporation GB100 [B200] [10de:2901] (rev a1)\n        Subsystem: NVIDIA Corporation Device [10de:1999]\n        Kernel driver in use: vfio-pci\n        Kernel modules: nvidiafb, nouveau\n"})}),(0,r.jsxs)(n.p,{children:["In this case, the device ID is ",(0,r.jsx)(n.code,{children:"2901"}),". The device ID then needs to be added to the list of supported GPUs:"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'device_id="2901" # replace with your device ID\ncurrent_ids=$(kubectl get daemonset nvidia-cc-manager -n gpu-operator -o jsonpath=\'{.spec.template.spec.containers[?(@.name=="nvidia-cc-manager")].env[?(@.name=="CC_CAPABLE_DEVICE_IDS")].value}\')\nkubectl set env daemonset/nvidia-cc-manager -n gpu-operator -c nvidia-cc-manager CC_CAPABLE_DEVICE_IDS="${current_ids},0x${device_id}"\n'})}),(0,r.jsx)(n.p,{children:"Then, the node feature rules need to be patched, so that the nodes with Blackwell GPUs are correctly recognized as nodes that can host GPU workloads.\nFor the B200 GPU shown above, the following values should be used:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GPU_NAME"}),": ",(0,r.jsx)(n.code,{children:"NVIDIA B200"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GPU_NAME_SHORT"}),": ",(0,r.jsx)(n.code,{children:"B200"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"DEVICE_ID"}),": ",(0,r.jsx)(n.code,{children:"2901"})]}),"\n"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'kubectl patch nodefeaturerule nvidia-nfd-nodefeaturerules --type=\'json\' -p=\'[\n  {\n    "op": "add",\n    "path": "/spec/rules/10",\n    "value": {\n      "name": "<GPU_NAME>",\n      "labels": {\n        "nvidia.com/gpu.<GPU_NAME_SHORT>": "true",\n        "nvidia.com/gpu.family": "blackwell"\n      },\n      "matchFeatures": [\n        {\n          "feature": "pci.device",\n          "matchExpressions": {\n            "device": {"op": "In", "value": ["<DEVICE_ID>"]},\n            "vendor": {"op": "In", "value": ["10de"]}\n          }\n        }\n      ]\n    }\n  },\n  {\n    "op": "add",\n    "path": "/spec/rules/11/matchAny/0/matchFeatures/0/matchExpressions/nvidia.com~1gpu.family/value/-",\n    "value": "blackwell"\n  },\n  {\n    "op": "add",\n    "path": "/spec/rules/11/matchAny/1/matchFeatures/0/matchExpressions/nvidia.com~1gpu.family/value/-",\n    "value": "blackwell"\n  }\n]\'\n'})})]}),"\n",(0,r.jsx)(n.p,{children:"Once the operator is fully deployed, which can take a few minutes, check the available GPUs in the cluster:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'kubectl get nodes -l nvidia.com/gpu.present -o json | \\\n  jq \'.items[0].status.allocatable |\n    with_entries(select(.key | startswith("nvidia.com/"))) |\n    with_entries(select(.value != "0"))\'\n'})}),"\n",(0,r.jsx)(n.p,{children:"The above command should yield an output similar to the following, depending on what GPUs are available:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "nvidia.com/pgpu": "1"\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["These identifiers are then used to ",(0,r.jsx)(n.a,{href:"/contrast/pr-preview/pr-2092/next/howto/workload-deployment/GPU-configuration",children:"run GPU workloads on the cluster"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}function u(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);